{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-Multiple_Regression - Lasso Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIKXvCOyeOyAvjuyg4A29o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patelyash9775/Advanced-Python/blob/main/Advance%20Regression/02_Multiple_Regression_Lasso_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suOjLVzbWg9X"
      },
      "source": [
        "## **TensorFlow 2: Advanced Linear & Lasso Regression in Python**\n",
        "\n",
        "*Topic: Advanced Linear Regression*\n",
        "  - *part-2: Feature Engineering*\n",
        "    - *Lesson: Multiple Linear Regression*\n",
        "    - *Lesson: LASSO Regression*\n",
        "    - *Lesson: Feature Selection based on Penality Factor*\n",
        "\n",
        "**PROJECT: CUSTOMER REVENUE PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzRoqSllYAUZ",
        "outputId": "f06cc097-b6e8-490d-c905-c445ce127e30"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7rVGVO-ZFfB"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD8zIT13ZJ_s"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tylue2z_ZUEy"
      },
      "source": [
        "os.chdir('drive/MyDrive/Advance Regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UulpCEabZwMb",
        "outputId": "15b4e3cc-e853-4360-d2a3-faddcae76f45"
      },
      "source": [
        "ls preprocessing/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_norm.pickle  y_norm.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mX2grzvZ43Q"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KesmbccJZ96q"
      },
      "source": [
        "\n",
        "### **Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO96qCWvaBxz"
      },
      "source": [
        "# loading preprocessed data\n",
        "x = pickle.load(open('preprocessing/x_norm.pickle','rb'))\n",
        "y = pickle.load(open('preprocessing/y_norm.pickle','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0e9CVbFaH1H"
      },
      "source": [
        "x.drop('index',axis=1,inplace=True) # drop index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG6BV2M2aPd_"
      },
      "source": [
        "### **Multiple Linear Regression**\n",
        "$\\hat y = a + b_1 X_1 + b_2 X_2 + ... + b_n X_n $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYEJ3TDoadgC"
      },
      "source": [
        "# convert into array\n",
        "x = x.values\n",
        "y = y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toTfD0YWalp1",
        "outputId": "d650cca2-1e60-4cbe-9b8a-cb13d22f9c24"
      },
      "source": [
        "x.shape , y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3189, 13), (3189, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pKFjJu7a2eu"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3x8ARuPa8MK"
      },
      "source": [
        "# multiple linear regression\n",
        "def multi_regression():\n",
        "  model = Sequential([\n",
        "                      layers.Dense(units=1,input_shape=(13,))\n",
        "  ])\n",
        "  # \n",
        "  loss = tf.keras.losses.mean_squared_error\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  # complie\n",
        "  model.compile(loss=loss,optimizer=optimizer,metrics=['mse'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEIMdwRtcuGe",
        "outputId": "53c80b84-6f12-4175-db99-b21d90fb259b"
      },
      "source": [
        "model = multi_regression()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 14        \n",
            "=================================================================\n",
            "Total params: 14\n",
            "Trainable params: 14\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDVxKqnvc2bS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bjbnf64dMM7",
        "outputId": "708037af-4c26-4e77-d5fc-7a816633105f"
      },
      "source": [
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2551, 13), (638, 13), (2551, 1), (638, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iLOvWI6dqIl",
        "outputId": "508d080f-ff28-4768-a09b-f1a042d318b0"
      },
      "source": [
        "#train\n",
        "history = model.fit(x_train,y_train,batch_size=100,epochs=200,validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 2.3612 - mse: 2.3612 - val_loss: 2.2945 - val_mse: 2.2945\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 2.0613 - mse: 2.0613 - val_loss: 1.9884 - val_mse: 1.9884\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.8020 - mse: 1.8020 - val_loss: 1.7334 - val_mse: 1.7334\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.5831 - mse: 1.5831 - val_loss: 1.5184 - val_mse: 1.5184\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.3964 - mse: 1.3964 - val_loss: 1.3380 - val_mse: 1.3380\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.2388 - mse: 1.2388 - val_loss: 1.1880 - val_mse: 1.1880\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.1052 - mse: 1.1052 - val_loss: 1.0633 - val_mse: 1.0633\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9941 - mse: 0.9941 - val_loss: 0.9576 - val_mse: 0.9576\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9010 - mse: 0.9010 - val_loss: 0.8702 - val_mse: 0.8702\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8229 - mse: 0.8229 - val_loss: 0.7992 - val_mse: 0.7992\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7601 - mse: 0.7601 - val_loss: 0.7398 - val_mse: 0.7398\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7077 - mse: 0.7077 - val_loss: 0.6921 - val_mse: 0.6921\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6640 - mse: 0.6640 - val_loss: 0.6529 - val_mse: 0.6529\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6286 - mse: 0.6286 - val_loss: 0.6186 - val_mse: 0.6186\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5991 - mse: 0.5991 - val_loss: 0.5923 - val_mse: 0.5923\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5744 - mse: 0.5744 - val_loss: 0.5700 - val_mse: 0.5700\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5546 - mse: 0.5546 - val_loss: 0.5509 - val_mse: 0.5509\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5372 - mse: 0.5372 - val_loss: 0.5357 - val_mse: 0.5357\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.5228 - mse: 0.5228 - val_loss: 0.5234 - val_mse: 0.5234\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5102 - mse: 0.5102 - val_loss: 0.5104 - val_mse: 0.5104\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4990 - mse: 0.4990 - val_loss: 0.4999 - val_mse: 0.4999\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4893 - mse: 0.4893 - val_loss: 0.4916 - val_mse: 0.4916\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4809 - mse: 0.4809 - val_loss: 0.4828 - val_mse: 0.4828\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4727 - mse: 0.4727 - val_loss: 0.4759 - val_mse: 0.4759\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4656 - mse: 0.4656 - val_loss: 0.4678 - val_mse: 0.4678\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4588 - mse: 0.4588 - val_loss: 0.4622 - val_mse: 0.4622\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4526 - mse: 0.4526 - val_loss: 0.4559 - val_mse: 0.4559\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4467 - mse: 0.4467 - val_loss: 0.4507 - val_mse: 0.4507\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4416 - mse: 0.4416 - val_loss: 0.4452 - val_mse: 0.4452\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4362 - mse: 0.4362 - val_loss: 0.4414 - val_mse: 0.4414\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4314 - mse: 0.4314 - val_loss: 0.4359 - val_mse: 0.4359\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4266 - mse: 0.4266 - val_loss: 0.4319 - val_mse: 0.4319\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4219 - mse: 0.4219 - val_loss: 0.4274 - val_mse: 0.4274\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4176 - mse: 0.4176 - val_loss: 0.4237 - val_mse: 0.4237\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4133 - mse: 0.4133 - val_loss: 0.4191 - val_mse: 0.4191\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4091 - mse: 0.4091 - val_loss: 0.4152 - val_mse: 0.4152\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4054 - mse: 0.4054 - val_loss: 0.4112 - val_mse: 0.4112\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4013 - mse: 0.4013 - val_loss: 0.4084 - val_mse: 0.4084\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3977 - mse: 0.3977 - val_loss: 0.4048 - val_mse: 0.4048\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3941 - mse: 0.3941 - val_loss: 0.4017 - val_mse: 0.4017\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3906 - mse: 0.3906 - val_loss: 0.3982 - val_mse: 0.3982\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3875 - mse: 0.3875 - val_loss: 0.3957 - val_mse: 0.3957\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3842 - mse: 0.3842 - val_loss: 0.3919 - val_mse: 0.3919\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3808 - mse: 0.3808 - val_loss: 0.3882 - val_mse: 0.3882\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3780 - mse: 0.3780 - val_loss: 0.3865 - val_mse: 0.3865\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3747 - mse: 0.3747 - val_loss: 0.3839 - val_mse: 0.3839\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3719 - mse: 0.3719 - val_loss: 0.3814 - val_mse: 0.3814\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3692 - mse: 0.3692 - val_loss: 0.3788 - val_mse: 0.3788\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3667 - mse: 0.3667 - val_loss: 0.3757 - val_mse: 0.3757\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3639 - mse: 0.3639 - val_loss: 0.3737 - val_mse: 0.3737\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3612 - mse: 0.3612 - val_loss: 0.3717 - val_mse: 0.3717\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3589 - mse: 0.3589 - val_loss: 0.3698 - val_mse: 0.3698\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3565 - mse: 0.3565 - val_loss: 0.3675 - val_mse: 0.3675\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3542 - mse: 0.3542 - val_loss: 0.3650 - val_mse: 0.3650\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3519 - mse: 0.3519 - val_loss: 0.3630 - val_mse: 0.3630\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3499 - mse: 0.3499 - val_loss: 0.3617 - val_mse: 0.3617\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3477 - mse: 0.3477 - val_loss: 0.3592 - val_mse: 0.3592\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3457 - mse: 0.3457 - val_loss: 0.3576 - val_mse: 0.3576\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3437 - mse: 0.3437 - val_loss: 0.3562 - val_mse: 0.3562\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3419 - mse: 0.3419 - val_loss: 0.3540 - val_mse: 0.3540\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3401 - mse: 0.3401 - val_loss: 0.3526 - val_mse: 0.3526\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3382 - mse: 0.3382 - val_loss: 0.3512 - val_mse: 0.3512\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3366 - mse: 0.3366 - val_loss: 0.3491 - val_mse: 0.3491\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3349 - mse: 0.3349 - val_loss: 0.3483 - val_mse: 0.3483\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3333 - mse: 0.3333 - val_loss: 0.3469 - val_mse: 0.3469\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3318 - mse: 0.3318 - val_loss: 0.3453 - val_mse: 0.3453\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3302 - mse: 0.3302 - val_loss: 0.3442 - val_mse: 0.3442\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3290 - mse: 0.3290 - val_loss: 0.3434 - val_mse: 0.3434\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3273 - mse: 0.3273 - val_loss: 0.3414 - val_mse: 0.3414\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3259 - mse: 0.3259 - val_loss: 0.3408 - val_mse: 0.3408\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3246 - mse: 0.3246 - val_loss: 0.3388 - val_mse: 0.3388\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3232 - mse: 0.3232 - val_loss: 0.3383 - val_mse: 0.3383\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3220 - mse: 0.3220 - val_loss: 0.3372 - val_mse: 0.3372\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3208 - mse: 0.3208 - val_loss: 0.3356 - val_mse: 0.3356\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3196 - mse: 0.3196 - val_loss: 0.3349 - val_mse: 0.3349\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3186 - mse: 0.3186 - val_loss: 0.3343 - val_mse: 0.3343\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3174 - mse: 0.3174 - val_loss: 0.3335 - val_mse: 0.3335\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3163 - mse: 0.3163 - val_loss: 0.3326 - val_mse: 0.3326\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3155 - mse: 0.3155 - val_loss: 0.3313 - val_mse: 0.3313\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3144 - mse: 0.3144 - val_loss: 0.3310 - val_mse: 0.3310\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3134 - mse: 0.3134 - val_loss: 0.3300 - val_mse: 0.3300\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3125 - mse: 0.3125 - val_loss: 0.3287 - val_mse: 0.3287\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3115 - mse: 0.3115 - val_loss: 0.3282 - val_mse: 0.3282\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3108 - mse: 0.3108 - val_loss: 0.3274 - val_mse: 0.3274\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3098 - mse: 0.3098 - val_loss: 0.3267 - val_mse: 0.3267\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3091 - mse: 0.3091 - val_loss: 0.3258 - val_mse: 0.3258\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3083 - mse: 0.3083 - val_loss: 0.3257 - val_mse: 0.3257\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3075 - mse: 0.3075 - val_loss: 0.3251 - val_mse: 0.3251\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3067 - mse: 0.3067 - val_loss: 0.3242 - val_mse: 0.3242\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3059 - mse: 0.3059 - val_loss: 0.3236 - val_mse: 0.3236\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3052 - mse: 0.3052 - val_loss: 0.3236 - val_mse: 0.3236\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3049 - mse: 0.3049 - val_loss: 0.3226 - val_mse: 0.3226\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3038 - mse: 0.3038 - val_loss: 0.3224 - val_mse: 0.3224\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3034 - mse: 0.3034 - val_loss: 0.3222 - val_mse: 0.3222\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3026 - mse: 0.3026 - val_loss: 0.3216 - val_mse: 0.3216\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3019 - mse: 0.3019 - val_loss: 0.3204 - val_mse: 0.3204\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3020 - mse: 0.3020 - val_loss: 0.3198 - val_mse: 0.3198\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3009 - mse: 0.3009 - val_loss: 0.3194 - val_mse: 0.3194\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3004 - mse: 0.3004 - val_loss: 0.3191 - val_mse: 0.3191\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2998 - mse: 0.2998 - val_loss: 0.3188 - val_mse: 0.3188\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2993 - mse: 0.2993 - val_loss: 0.3180 - val_mse: 0.3180\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2987 - mse: 0.2987 - val_loss: 0.3178 - val_mse: 0.3178\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2983 - mse: 0.2983 - val_loss: 0.3185 - val_mse: 0.3185\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2976 - mse: 0.2976 - val_loss: 0.3173 - val_mse: 0.3173\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2973 - mse: 0.2973 - val_loss: 0.3168 - val_mse: 0.3168\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2968 - mse: 0.2968 - val_loss: 0.3160 - val_mse: 0.3160\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2964 - mse: 0.2964 - val_loss: 0.3157 - val_mse: 0.3157\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2961 - mse: 0.2961 - val_loss: 0.3162 - val_mse: 0.3162\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2954 - mse: 0.2954 - val_loss: 0.3156 - val_mse: 0.3156\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2952 - mse: 0.2952 - val_loss: 0.3148 - val_mse: 0.3148\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2949 - mse: 0.2949 - val_loss: 0.3152 - val_mse: 0.3152\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2943 - mse: 0.2943 - val_loss: 0.3144 - val_mse: 0.3144\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2939 - mse: 0.2939 - val_loss: 0.3140 - val_mse: 0.3140\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2937 - mse: 0.2937 - val_loss: 0.3138 - val_mse: 0.3138\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2933 - mse: 0.2933 - val_loss: 0.3141 - val_mse: 0.3141\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2929 - mse: 0.2929 - val_loss: 0.3132 - val_mse: 0.3132\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2926 - mse: 0.2926 - val_loss: 0.3136 - val_mse: 0.3136\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2919 - mse: 0.2919 - val_loss: 0.3125 - val_mse: 0.3125\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2918 - mse: 0.2918 - val_loss: 0.3123 - val_mse: 0.3123\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2914 - mse: 0.2914 - val_loss: 0.3123 - val_mse: 0.3123\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2914 - mse: 0.2914 - val_loss: 0.3118 - val_mse: 0.3118\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2908 - mse: 0.2908 - val_loss: 0.3121 - val_mse: 0.3121\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2905 - mse: 0.2905 - val_loss: 0.3113 - val_mse: 0.3113\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2901 - mse: 0.2901 - val_loss: 0.3115 - val_mse: 0.3115\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2900 - mse: 0.2900 - val_loss: 0.3116 - val_mse: 0.3116\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2895 - mse: 0.2895 - val_loss: 0.3109 - val_mse: 0.3109\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2893 - mse: 0.2893 - val_loss: 0.3102 - val_mse: 0.3102\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2893 - mse: 0.2893 - val_loss: 0.3110 - val_mse: 0.3110\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.3104 - val_mse: 0.3104\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2886 - mse: 0.2886 - val_loss: 0.3108 - val_mse: 0.3108\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2882 - mse: 0.2882 - val_loss: 0.3099 - val_mse: 0.3099\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2881 - mse: 0.2881 - val_loss: 0.3098 - val_mse: 0.3098\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2877 - mse: 0.2877 - val_loss: 0.3096 - val_mse: 0.3096\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2874 - mse: 0.2874 - val_loss: 0.3097 - val_mse: 0.3097\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2872 - mse: 0.2872 - val_loss: 0.3096 - val_mse: 0.3096\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2871 - mse: 0.2871 - val_loss: 0.3087 - val_mse: 0.3087\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2868 - mse: 0.2868 - val_loss: 0.3094 - val_mse: 0.3094\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2864 - mse: 0.2864 - val_loss: 0.3090 - val_mse: 0.3090\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2863 - mse: 0.2863 - val_loss: 0.3086 - val_mse: 0.3086\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2859 - mse: 0.2859 - val_loss: 0.3084 - val_mse: 0.3084\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2861 - mse: 0.2861 - val_loss: 0.3086 - val_mse: 0.3086\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2857 - mse: 0.2857 - val_loss: 0.3082 - val_mse: 0.3082\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2855 - mse: 0.2855 - val_loss: 0.3079 - val_mse: 0.3079\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2853 - mse: 0.2853 - val_loss: 0.3080 - val_mse: 0.3080\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2849 - mse: 0.2849 - val_loss: 0.3079 - val_mse: 0.3079\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2850 - mse: 0.2850 - val_loss: 0.3073 - val_mse: 0.3073\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2847 - mse: 0.2847 - val_loss: 0.3072 - val_mse: 0.3072\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2844 - mse: 0.2844 - val_loss: 0.3075 - val_mse: 0.3075\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2844 - mse: 0.2844 - val_loss: 0.3078 - val_mse: 0.3078\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2843 - mse: 0.2843 - val_loss: 0.3068 - val_mse: 0.3068\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2839 - mse: 0.2839 - val_loss: 0.3068 - val_mse: 0.3068\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2841 - mse: 0.2841 - val_loss: 0.3064 - val_mse: 0.3064\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2836 - mse: 0.2836 - val_loss: 0.3071 - val_mse: 0.3071\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2834 - mse: 0.2834 - val_loss: 0.3068 - val_mse: 0.3068\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2833 - mse: 0.2833 - val_loss: 0.3060 - val_mse: 0.3060\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2831 - mse: 0.2831 - val_loss: 0.3066 - val_mse: 0.3066\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2829 - mse: 0.2829 - val_loss: 0.3065 - val_mse: 0.3065\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2826 - mse: 0.2826 - val_loss: 0.3059 - val_mse: 0.3059\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2826 - mse: 0.2826 - val_loss: 0.3054 - val_mse: 0.3054\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2826 - mse: 0.2826 - val_loss: 0.3052 - val_mse: 0.3052\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2825 - mse: 0.2825 - val_loss: 0.3061 - val_mse: 0.3061\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2822 - mse: 0.2822 - val_loss: 0.3051 - val_mse: 0.3051\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2819 - mse: 0.2819 - val_loss: 0.3053 - val_mse: 0.3053\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2821 - mse: 0.2821 - val_loss: 0.3055 - val_mse: 0.3055\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2815 - mse: 0.2815 - val_loss: 0.3049 - val_mse: 0.3049\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2817 - mse: 0.2817 - val_loss: 0.3052 - val_mse: 0.3052\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2814 - mse: 0.2814 - val_loss: 0.3048 - val_mse: 0.3048\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2812 - mse: 0.2812 - val_loss: 0.3051 - val_mse: 0.3051\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2813 - mse: 0.2813 - val_loss: 0.3054 - val_mse: 0.3054\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2809 - mse: 0.2809 - val_loss: 0.3046 - val_mse: 0.3046\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2808 - mse: 0.2808 - val_loss: 0.3041 - val_mse: 0.3041\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2807 - mse: 0.2807 - val_loss: 0.3043 - val_mse: 0.3043\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2807 - mse: 0.2807 - val_loss: 0.3041 - val_mse: 0.3041\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2806 - mse: 0.2806 - val_loss: 0.3045 - val_mse: 0.3045\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2803 - mse: 0.2803 - val_loss: 0.3045 - val_mse: 0.3045\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2803 - mse: 0.2803 - val_loss: 0.3044 - val_mse: 0.3044\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2803 - mse: 0.2803 - val_loss: 0.3034 - val_mse: 0.3034\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2804 - mse: 0.2804 - val_loss: 0.3038 - val_mse: 0.3038\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2800 - mse: 0.2800 - val_loss: 0.3036 - val_mse: 0.3036\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2800 - mse: 0.2800 - val_loss: 0.3042 - val_mse: 0.3042\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2799 - mse: 0.2799 - val_loss: 0.3030 - val_mse: 0.3030\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2796 - mse: 0.2796 - val_loss: 0.3036 - val_mse: 0.3036\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2795 - mse: 0.2795 - val_loss: 0.3037 - val_mse: 0.3037\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2796 - mse: 0.2796 - val_loss: 0.3039 - val_mse: 0.3039\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2794 - mse: 0.2794 - val_loss: 0.3030 - val_mse: 0.3030\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2794 - mse: 0.2794 - val_loss: 0.3035 - val_mse: 0.3035\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.3035 - val_mse: 0.3035\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.3033 - val_mse: 0.3033\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2793 - mse: 0.2793 - val_loss: 0.3032 - val_mse: 0.3032\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.3030 - val_mse: 0.3030\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2788 - mse: 0.2788 - val_loss: 0.3030 - val_mse: 0.3030\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2787 - mse: 0.2787 - val_loss: 0.3033 - val_mse: 0.3033\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2786 - mse: 0.2786 - val_loss: 0.3035 - val_mse: 0.3035\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2786 - mse: 0.2786 - val_loss: 0.3029 - val_mse: 0.3029\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2789 - mse: 0.2789 - val_loss: 0.3032 - val_mse: 0.3032\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2784 - mse: 0.2784 - val_loss: 0.3024 - val_mse: 0.3024\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2785 - mse: 0.2785 - val_loss: 0.3024 - val_mse: 0.3024\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2784 - mse: 0.2784 - val_loss: 0.3029 - val_mse: 0.3029\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.2781 - mse: 0.2781 - val_loss: 0.3033 - val_mse: 0.3033\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2780 - mse: 0.2780 - val_loss: 0.3028 - val_mse: 0.3028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQVIz-Xoe6t1"
      },
      "source": [
        "hist = history.history\n",
        "df_hist = pd.DataFrame(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "D1jDm_3UfHm2",
        "outputId": "c800cebc-8bc8-42cf-d2f8-34b2b8d8a002"
      },
      "source": [
        "df_hist[['loss','val_loss']].plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f903e2a2b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfHOO_r4g4an"
      },
      "source": [
        "### **LASSO Regression**\n",
        "- L1 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxuox71Jg6A2"
      },
      "source": [
        "from tensorflow.keras.regularizers import l1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7p43kE0s2Az"
      },
      "source": [
        "def lasso_regression(penality):\n",
        "  model = Sequential([\n",
        "                      layers.Dense(units=1,input_shape=(13,),kernel_regularizer=l1(penality))\n",
        "  ])\n",
        "\n",
        "  #loss and optimizer\n",
        "  loss = tf.keras.losses.mean_squared_error\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  # compile\n",
        "  model.compile(optimizer=optimizer,loss=loss)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcKc5gcPt434"
      },
      "source": [
        "# different penality factors\n",
        "penality = [1e-10,3e-10,6e-10,9e-10,\n",
        "            1e-9,3e-9,6e-9,9e-9,\n",
        "            1e-8,3e-8,6e-8,9e-8,\n",
        "            1e-7,3e-7,6e-7,9e-7,\n",
        "            1e-6,3e-6,6e-6,9e-6,\n",
        "            1e-5,3e-5,6e-5,9e-5,\n",
        "            1e-4,3e-4,6e-4,9e-4,\n",
        "            1e-3,3e-3,6e-3,9e-3,\n",
        "            1e-2,3e-2,6e-2,9e-2,\n",
        "            1e-1,3e-1,6e-1,1,3,6,10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BDSaABtnuEdM",
        "outputId": "deffd350-bc0a-4eac-d49b-e2f3815bc266"
      },
      "source": [
        "# training model for each penality factor\n",
        "loss_values = [] \n",
        "for lam in penality:\n",
        "  print('Running the model for lambda = %f'%lam)\n",
        "  # initilizing model with lambda\n",
        "  model_lasso = lasso_regression(lam)\n",
        "  # fitting the model\n",
        "  history = model_lasso.fit(x_train,y_train,batch_size=100,epochs=500,validation_data=(x_test,y_test))\n",
        "  # visualize loss for different iteration or epochs\n",
        "  df_history = pd.DataFrame(history.history)\n",
        "  df_history[['loss','val_loss']].plot()\n",
        "  plt.show()\n",
        "  #  save the weight \n",
        "  weights = model_lasso.get_weights() # coloumn \n",
        "  try:\n",
        "    weight_penality = np.concatenate((weight_penality,weights[0].T),axis=0)\n",
        "  except:\n",
        "    weight_penality = weights[0].T # rows\n",
        "  # loss\n",
        "  loss_values.append(df_history.iloc[-1].to_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running the model for lambda = 0.000000\n",
            "Epoch 1/500\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 1.5071 - val_loss: 1.3655\n",
            "Epoch 2/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.4066 - val_loss: 1.2771\n",
            "Epoch 3/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.3129 - val_loss: 1.1979\n",
            "Epoch 4/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2281 - val_loss: 1.1248\n",
            "Epoch 5/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.1496 - val_loss: 1.0564\n",
            "Epoch 6/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0777 - val_loss: 0.9927\n",
            "Epoch 7/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.0105 - val_loss: 0.9347\n",
            "Epoch 8/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.9495 - val_loss: 0.8791\n",
            "Epoch 9/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8917 - val_loss: 0.8303\n",
            "Epoch 10/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.8391 - val_loss: 0.7858\n",
            "Epoch 11/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.7910 - val_loss: 0.7425\n",
            "Epoch 12/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7460 - val_loss: 0.7040\n",
            "Epoch 13/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7049 - val_loss: 0.6675\n",
            "Epoch 14/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6674 - val_loss: 0.6348\n",
            "Epoch 15/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.6326 - val_loss: 0.6043\n",
            "Epoch 16/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.6012 - val_loss: 0.5770\n",
            "Epoch 17/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5718 - val_loss: 0.5518\n",
            "Epoch 18/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5456 - val_loss: 0.5295\n",
            "Epoch 19/500\n",
            "26/26 [==============================] - 0s 14ms/step - loss: 0.5217 - val_loss: 0.5077\n",
            "Epoch 20/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4993 - val_loss: 0.4897\n",
            "Epoch 21/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4798 - val_loss: 0.4722\n",
            "Epoch 22/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4618 - val_loss: 0.4566\n",
            "Epoch 23/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.4458 - val_loss: 0.4429\n",
            "Epoch 24/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.4305\n",
            "Epoch 25/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4182 - val_loss: 0.4185\n",
            "Epoch 26/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4064 - val_loss: 0.4091\n",
            "Epoch 27/500\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3958 - val_loss: 0.3994\n",
            "Epoch 28/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3862 - val_loss: 0.3916\n",
            "Epoch 29/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3778 - val_loss: 0.3841\n",
            "Epoch 30/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3700 - val_loss: 0.3778\n",
            "Epoch 31/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3630 - val_loss: 0.3718\n",
            "Epoch 32/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3571 - val_loss: 0.3663\n",
            "Epoch 33/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3517 - val_loss: 0.3624\n",
            "Epoch 34/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3465 - val_loss: 0.3573\n",
            "Epoch 35/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3418 - val_loss: 0.3532\n",
            "Epoch 36/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3381 - val_loss: 0.3492\n",
            "Epoch 37/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3340 - val_loss: 0.3466\n",
            "Epoch 38/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3310 - val_loss: 0.3434\n",
            "Epoch 39/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3277 - val_loss: 0.3414\n",
            "Epoch 40/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3251 - val_loss: 0.3387\n",
            "Epoch 41/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3225 - val_loss: 0.3365\n",
            "Epoch 42/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3200 - val_loss: 0.3347\n",
            "Epoch 43/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3181 - val_loss: 0.3322\n",
            "Epoch 44/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3165 - val_loss: 0.3304\n",
            "Epoch 45/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3143 - val_loss: 0.3292\n",
            "Epoch 46/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 0.3276\n",
            "Epoch 47/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3110 - val_loss: 0.3265\n",
            "Epoch 48/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3094 - val_loss: 0.3247\n",
            "Epoch 49/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.3237\n",
            "Epoch 50/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3069 - val_loss: 0.3223\n",
            "Epoch 51/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3057 - val_loss: 0.3215\n",
            "Epoch 52/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3045 - val_loss: 0.3207\n",
            "Epoch 53/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3035 - val_loss: 0.3196\n",
            "Epoch 54/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3024 - val_loss: 0.3190\n",
            "Epoch 55/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3016 - val_loss: 0.3182\n",
            "Epoch 56/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3005 - val_loss: 0.3170\n",
            "Epoch 57/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2998 - val_loss: 0.3162\n",
            "Epoch 58/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2988 - val_loss: 0.3155\n",
            "Epoch 59/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2981 - val_loss: 0.3148\n",
            "Epoch 60/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2972 - val_loss: 0.3147\n",
            "Epoch 61/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2966 - val_loss: 0.3139\n",
            "Epoch 62/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2958 - val_loss: 0.3134\n",
            "Epoch 63/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2954 - val_loss: 0.3125\n",
            "Epoch 64/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2947 - val_loss: 0.3126\n",
            "Epoch 65/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2942 - val_loss: 0.3119\n",
            "Epoch 66/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2934 - val_loss: 0.3113\n",
            "Epoch 67/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2929 - val_loss: 0.3112\n",
            "Epoch 68/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2924 - val_loss: 0.3105\n",
            "Epoch 69/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2918 - val_loss: 0.3097\n",
            "Epoch 70/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2913 - val_loss: 0.3093\n",
            "Epoch 71/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2909 - val_loss: 0.3094\n",
            "Epoch 72/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2909 - val_loss: 0.3086\n",
            "Epoch 73/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2903 - val_loss: 0.3088\n",
            "Epoch 74/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2896 - val_loss: 0.3085\n",
            "Epoch 75/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2893 - val_loss: 0.3085\n",
            "Epoch 76/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2889 - val_loss: 0.3080\n",
            "Epoch 77/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 0.3074\n",
            "Epoch 78/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2883 - val_loss: 0.3073\n",
            "Epoch 79/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2878 - val_loss: 0.3069\n",
            "Epoch 80/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2877 - val_loss: 0.3072\n",
            "Epoch 81/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2872 - val_loss: 0.3065\n",
            "Epoch 82/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2868 - val_loss: 0.3063\n",
            "Epoch 83/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2867 - val_loss: 0.3065\n",
            "Epoch 84/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2865 - val_loss: 0.3058\n",
            "Epoch 85/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2860 - val_loss: 0.3060\n",
            "Epoch 86/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2859 - val_loss: 0.3060\n",
            "Epoch 87/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2857 - val_loss: 0.3051\n",
            "Epoch 88/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2854 - val_loss: 0.3057\n",
            "Epoch 89/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2851 - val_loss: 0.3054\n",
            "Epoch 90/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2849 - val_loss: 0.3052\n",
            "Epoch 91/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2847 - val_loss: 0.3049\n",
            "Epoch 92/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2846 - val_loss: 0.3052\n",
            "Epoch 93/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2842 - val_loss: 0.3047\n",
            "Epoch 94/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2841 - val_loss: 0.3054\n",
            "Epoch 95/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2839 - val_loss: 0.3041\n",
            "Epoch 96/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2837 - val_loss: 0.3044\n",
            "Epoch 97/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2835 - val_loss: 0.3040\n",
            "Epoch 98/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2834 - val_loss: 0.3037\n",
            "Epoch 99/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2831 - val_loss: 0.3038\n",
            "Epoch 100/500\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2829 - val_loss: 0.3038\n",
            "Epoch 101/500\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.2828 - val_loss: 0.3036\n",
            "Epoch 102/500\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.3145"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-dcd7094cc411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmodel_lasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# visualize loss for different iteration or epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mdf_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3ts0Rb_xV38"
      },
      "source": [
        "### **Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTGYiY10xW48"
      },
      "source": [
        "log_lambda=np.log10(penality)\n",
        "#weight_penality\n",
        "df_weights = pd.DataFrame(weight_penality)\n",
        "# loss\n",
        "df_loss = pd.DataFrame(loss_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ve7fdwPzyi_"
      },
      "source": [
        "**Visulization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1C6ox1P0zHL",
        "outputId": "48cc1311-589c-4296-c7a0-b34feb5e91b6"
      },
      "source": [
        "df_weights.iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0.050898\n",
              "1     0.073927\n",
              "2    -0.033329\n",
              "3    -0.019556\n",
              "4     1.048638\n",
              "5     0.821342\n",
              "6     0.037994\n",
              "7    -0.005354\n",
              "8    -1.211219\n",
              "9    -0.121205\n",
              "10    0.169702\n",
              "11   -0.107160\n",
              "12   -0.214061\n",
              "Name: 0, dtype: float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-7DDQZv07dK",
        "outputId": "c43203a6-9450-41bc-9bfd-0a2cf386e05e"
      },
      "source": [
        "list(log_lambda).index(-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "RV-PTBaCz3vC",
        "outputId": "db035c9e-41fd-4cb7-df73-6c9e4b51b5ef"
      },
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "plt.subplot(1,2,1),\n",
        "plt.plot(log_lambda,df_weights)\n",
        "#lablel\n",
        "plt.xlabel('Log_Lambda')\n",
        "plt.ylabel('Coeffients_weights')\n",
        "plt.grid()\n",
        "# weight number\n",
        "for i in range(13):\n",
        "  plt.text(-10,df_weights.iloc[0][i],'%d'%i)\n",
        "\n",
        "plt.plot([-2,-2],[-1.1,1.1],'k:')\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "ply.plot(log_lamda,df_loss['val_loss'])\n",
        "#lablel\n",
        "plt.xlabel('Log_Lambda')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.plot([-2,-2],[0.3,1.1],'k:')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-ce46f0c537d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#lablel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Log_Lambda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                     message='Support for multi-dimensional indexing')\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m                 \u001b[0;31m# we have definitely hit a pandas index or series object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;31m# cast to a numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (slice(None, None, None), None)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyxjDbjgBrSL",
        "outputId": "d9f77ad4-c3f3-4195-9ae2-d1a06a73241d"
      },
      "source": [
        "plt.figure(figsize=(10,50))\n",
        "plt.plot(log_lambda,df_weights)\n",
        "# label\n",
        "plt.xlabel('Log Lambda')\n",
        "plt.ylabel('Coeffients weights')\n",
        "plt.grid()\n",
        "# weight number\n",
        "for i in range(13):\n",
        "  plt.text(-2,df_weights.iloc[32][i],'%d'%i)\n",
        "\n",
        "plt.plot([-2,-2],[-1.1,1.1],'k:')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-1e832e9cd505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Log Lambda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Coeffients weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                     message='Support for multi-dimensional indexing')\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m                 \u001b[0;31m# we have definitely hit a pandas index or series object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;31m# cast to a numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (slice(None, None, None), None)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x3600 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxl3R4COCLuP"
      },
      "source": [
        "**Feature selected**\n",
        "\n",
        "*** : 8,4,5\n",
        "\n",
        "**: 10\n",
        "\n",
        "*: 1, 0, 6, 7, 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP3ws5lCC_Ku"
      },
      "source": [
        "x = pickle.load(open('preprocessing/x_norm.pickle','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hic6ReRVDCmw"
      },
      "source": [
        "x.drop('index',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDdqdH5MDFUQ",
        "outputId": "865706f3-da24-4986-8d77-17e75fb5340c"
      },
      "source": [
        "x.iloc[:,[8,4,5,10,1,0,6,7,9]].columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['NoOfGamesBought', 'FrquncyOfPurchase', 'NoOfUnitsPurchased',\n",
              "       'fct_Favorite', 'MinAgeOfChild', 'NoOfChildren', 'FrequencyOFPlay',\n",
              "       'NoOfGamesPlayed', 'city_1'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    }
  ]
}